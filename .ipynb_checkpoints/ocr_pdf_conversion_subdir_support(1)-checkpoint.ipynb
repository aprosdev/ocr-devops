{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84b1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE copy of ocr_pdf_conversion_minimal.py but now supports ocr on each subdir as a municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89bfc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\data_bindu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\data_bindu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import difflib\n",
    "\n",
    "from pprint import pprint\n",
    "import io\n",
    "import re\n",
    "\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd18e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Variables to change: \n",
    "path_to_base_dir \n",
    "\"\"\"\n",
    "\n",
    "#subdir example\n",
    "path_to_base_dir = r\"C:\\Users\\Administrator\\Downloads\\Yorktown\\Yorktown\\Architectural Review Board\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92750e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_ocr_text(row):\n",
    "    \n",
    "    #if row.page_text == Nan then  row.page_text == row.page_text returns False so not makes it true so the program knows no text in that row.    \n",
    "    if row.page_text == \"nan\" or  row.page_text == \"\" or row.page_text.isspace():\n",
    "        return True#use OCR text because embedded doesn't have any text or text worth using( whitespace only)\n",
    "    else: \n",
    "        #embedded text can be used\n",
    "        #NOTE: this is where one could check if more OCR text rather than whatever is in embedded.\n",
    "        return False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4506ca50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mUsed to determine if rotating the PDF results in more correctly spelled english words, if so then \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mthe PDF needs to be rotated before applying OCR because the page format is setup differently.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mTODO: Potentially remove stop words from brown.words to not weight them heavily\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m word_list \u001b[38;5;241m=\u001b[39m \u001b[43mbrown\u001b[49m\u001b[38;5;241m.\u001b[39mwords()\n\u001b[0;32m      9\u001b[0m word_set_original \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(word_list)\n\u001b[0;32m     13\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brown' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Used to determine if rotating the PDF results in more correctly spelled english words, if so then \n",
    "the PDF needs to be rotated before applying OCR because the page format is setup differently.\n",
    "\n",
    "An attempt to maximize words captured with OCR.\n",
    "TODO: Potentially remove stop words from brown.words to not weight them heavily\n",
    "\"\"\"\n",
    "word_list = brown.words()\n",
    "word_set_original = set(word_list)\n",
    "\n",
    "\n",
    "\n",
    "stopwords =set(stopwords.words('english'))\n",
    "\n",
    "\"\"\"\n",
    "Note Brown corpus considers single character english tokens as words which might throw off word count if rotated valid word count is close to not rotated valid word count: B,o,b, S, every single character it seems.\n",
    "Removing below.\n",
    "\n",
    "TODO: Get a better word checking model/method. \n",
    "TODO: brown.words() still contains 2 char words which are\n",
    "old words but are more likely to be OCR error than actually occur in the document. \n",
    "For example:  en, ye, pa\n",
    "\n",
    "TODO: Could get all 2 character words from brown and remove the old ones to clean up the accepted OCR words. \n",
    "\"\"\"\n",
    "\n",
    "# only want to count 'a' and 'i' as valid single char word. Also removing ] [ _ etc\n",
    "#single_alphabetical_chars_to_not_count_as_valid_word = {'_','[',']','b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}#Otherwise Brown considers a-z{1} to be a valid \"word\"\n",
    "\n",
    "#without 'a' and 'i' because they over power actual words. These two can easily be OCR errors\n",
    "single_alphabetical_chars_to_not_count_as_valid_word = {'a','i','_','[',']','b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}#Otherwise Brown considers a-z{1} to be a valid \"word\"\n",
    "\n",
    "\n",
    "#removing certain words from brown words\n",
    "word_set=word_set_original - single_alphabetical_chars_to_not_count_as_valid_word\n",
    "print(\"word_set length with stopwords:{}\".format(len(word_set)))\n",
    "\n",
    "#removing stopwords from brown words\n",
    "#word_set_wo_stopwords=word_set_original - stopwords\n",
    "word_set=(word_set_original - stopwords) - single_alphabetical_chars_to_not_count_as_valid_word\n",
    "print(\"word_set length without stopwords:{}\".format(len(word_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77085daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "TODO: Might need to add below to env variables for setup to work. \n",
    "TESSDATA_PREFIX  C:\\Program Files\\Tesseract-OCR\\tessdata \n",
    "\n",
    "Note: All variables that need to be changed to point to input & output directories are below in this cell.\n",
    "\n",
    "Main variables to change: \n",
    "#dir containing municipalities as subfolders \n",
    "path_to_base_dir\n",
    "ocr_output_files_dir_name\n",
    "ocr_output_file_metadata_dir_name\n",
    "\n",
    "path_to_website_documents\n",
    "path_to_ocr_output_dir\n",
    "minimial_file_output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "website_files_to_ocr_dir_name = \"website_files_to_ocr\"\n",
    "#Base dir to search from for files\n",
    "#Note folders would have to be made already or add folder creation into code\n",
    "ocr_output_files_dir_name = \"ocr_conversion_folder\"\n",
    "ocr_output_file_metadata_dir_name= \"ocr_conversion_metadata_folder\"\n",
    "#NOTE: This would be the dir containing sub-folders/subdirectories each as one municipality containing all the scrapped documents and webpages .\n",
    "#path_to_base_dir = r\"C:\\Users\\data_bindu\\Documents\\welcomehomes Automation of municipalities rules\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_excel_file_name = \"ocred_website_files.xlsx\"#results of OCR on PDF pages. Not sure if this is needed because more complex page joins( embedded and OCR) are done to maximize text in PDF \n",
    "output_excel_file_metadata_name = \"ocred_website_files_metadata.xlsx\"\n",
    "\n",
    "output_excel_original_text_file_name = \"original_text_from_website_files.xlsx\"#text which was embedded in PDF( not a scan)\n",
    "\n",
    "output_excel_embedded_text_per_page_and_file_name = \"embedded_text_per_page_from_website_files.xlsx\"#text which was embedded in PDF( not a scan)\n",
    "\n",
    "#Contains the orientation_metric_ocred_page_text for all pages ( use_ocr_x == True or use_ocr_x ==False) not just the text pages which don't have embedded text i.e. use_ocr_x == True as seen in page_orientation_stats_extra.xlsx .\n",
    "output_excel_ocr_text_per_page_and_file_name = \"ocr_text_per_page_from_website_files.xlsx\"\n",
    "\n",
    "output_excel_merged_text_per_page_and_file_name= \"merged_text_per_page_from_website_files.xlsx\"#subset of pages needing ocr\n",
    "\n",
    "#NOTE: Don't use as input to model because excel cell has character limit which truncates text. Instead load in .txt file\n",
    "output_excel_merged_text_final_per_page_and_file_name = \"pdf_merged_text_per_page_from_website_files DONT USE TRUNCATES.xlsx\"#full df with all rows potentially join all pages into one df\n",
    "\n",
    "#page links found in the pdf. Context data/feature for model. Could lookup if page is captured on website page scrapper.\n",
    "output_excel_file_page_links = \"website_files_page_links.xlsx\"\n",
    "\n",
    "\n",
    "#Debug structure for valid words to ensure OCR page orientation is operating correctly.\n",
    "output_excel_valid_words_rotated_ocr_text_per_page_and_file_name = \"valid_words_rotated_ocr_text_per_page_from_website_files.xlsx\"#text which was embedded in PDF( not a scan)\n",
    "output_excel_valid_words_ocr_text_per_page_and_file_name = \"valid_words_ocr_text_per_page_from_website_files.xlsx\"#text which was embedded in PDF( not a scan)\n",
    "\n",
    "\n",
    "output_excel_file_rotated_ocr_text_name = \"rotated_ocr_text.xlsx\"\n",
    "output_excel_file_not_rotated_ocr_text_name = \"not_rotated_ocr_text.xlsx\"\n",
    "#Stats on which text was used for a given page: rotated page or not, and word counts for those pages.\n",
    "output_excel_file_page_orientation_stats = \"page_orientation_stats.xlsx\"\n",
    "output_excel_file_page_orientation_stats_extra = \"page_orientation_stats_extra.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_excel_file_per_page_merged_text_file_name = \"merged_text_final_df.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "#will not perform ocr on PDFs with more than this number of pages.\n",
    "#Will alter methods if a count is applied to a dir because this will filter some pdfs out of dir if longer than page limit\n",
    "#Not really needed because ocr_page_early_stop_limit could at least OCR this many pages and just use that to represent the PDF file rather than not applying OCR entirely \n",
    "ocr_page_count_limit = 4000\n",
    "\n",
    "#OCR stopping by only apply OCR to first n pages \n",
    "ocr_page_early_stop_limit = 400\n",
    "\n",
    "skipped_pdf_file_name = \"skipped_pdfs_too_long.xlsx\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NOTE: Keep minimial_file_output = False\n",
    "because some files here are needed for model. \n",
    "Page links or PDF metadata for example, and the other files are needed for review of OCR conversion\n",
    "\"\"\"\n",
    "\n",
    "minimial_file_output = False#debug mode, more files generated to see the steps/computation and why certain pages are being used, metadata from PDF, etc\n",
    "#minimial_file_output = True#output minimial files needed which is the ocr dir with plain text for each PDF file.\n",
    "\n",
    "\"\"\"\n",
    "Keep old_output = True, and could remove code/variables associated with this if need to improve speed/lower compute. \n",
    "\"\"\"\n",
    "#Can likely clean up code which relates to this field/flag\n",
    "#old_output = False#Include files which have been replaced by better methods/views of data.\n",
    "old_output = True#Don't include files which have been replaced by better methodsmethods/views of data.\n",
    "\n",
    "\n",
    "#Likely keep False unless to verify the joining process between files. If true then more files will be made which include redundant info\n",
    "#if true then see as seperate files prior to join, if false then don't write out files because the data is in another file. \n",
    "see_not_joined_files = False\n",
    "#see_not_joined_files = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d25fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m municipality_and_ocr_files_paths_dict \u001b[38;5;241m=\u001b[39m {}\u001b[38;5;66;03m#key = path to base of a single municipality, value = path to municipality/website_files_to_ocr\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mCreating output folders for OCR results and metadata\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m municipality \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(path_to_base_dir):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#Base of folder. Used to create output folders.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     path_to_municipality \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path_to_base_dir,municipality)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_municipality=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path_to_municipality))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attempt with os.listdir after gathering all ocr_folders \n",
    "with path_to_website_files_to_ocr = r\"{}\\{}\".format(path_to_municipality,website_files_to_ocr_dir_name)\n",
    "Uses the fact that the folder will be named  website_files_to_ocr_dir_name = \"website_files_to_ocr\"\n",
    "\"\"\"\n",
    "\n",
    "ocr_folder_paths = []\n",
    "municipality_and_ocr_files_paths_dict = {}#key = path to base of a single municipality, value = path to municipality/website_files_to_ocr\n",
    "\"\"\"\n",
    "Creating output folders for OCR results and metadata\n",
    "\"\"\"\n",
    "for municipality in os.listdir(path_to_base_dir):\n",
    "    #Base of folder. Used to create output folders.\n",
    "    path_to_municipality = r\"{}\\{}\".format(path_to_base_dir,municipality)\n",
    "    print(\"path_to_municipality={}\\n\".format(path_to_municipality))\n",
    "    #Folder with all OCRED files one txt file for each input PDF. \n",
    "    path_to_ocr_output_dir = r\"{}\\{}\".format(path_to_municipality,ocr_output_files_dir_name)\n",
    "    print(\"path_to_ocr_output_dir={}\\n\".format(path_to_ocr_output_dir))\n",
    "    \n",
    "    \n",
    "    path_to_website_files_to_ocr = r\"{}\\{}\".format(path_to_municipality,website_files_to_ocr_dir_name)\n",
    "    municipality_and_ocr_files_paths_dict[path_to_municipality] = path_to_website_files_to_ocr\n",
    "    print(\"path_to_website_files_to_ocr={}\\n\".format(path_to_website_files_to_ocr))\n",
    "\n",
    "    #Folder to store metadata collected when converting PDFs into plain text.\n",
    "    path_to_ocr_metadata_output_dir = r\"{}\\{}\".format(path_to_municipality,ocr_output_file_metadata_dir_name)\n",
    "    print(\"path_to_ocr_metadata_output_dir={}\\n\".format(path_to_ocr_metadata_output_dir))\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path_to_ocr_output_dir)\n",
    "        os.mkdir(path_to_ocr_metadata_output_dir)\n",
    "\n",
    "    except FileExistsError:\n",
    "        print(\"Dir already made\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for base_municipality_path,ocr_folder_path in municipality_and_ocr_files_paths_dict.items():\n",
    "    #dir_name = strip right most from ocr_folder_path\n",
    "    #norm_ocr_folder_path = os.path.normpath(ocr_folder_path)\n",
    "    #dir_name = os.path.basename(norm_ocr_folder_path)\n",
    "    path_to_municipality = r\"{}\".format(base_municipality_path)\n",
    "    print(\"path_to_municipality={}\\n\".format(path_to_municipality))\n",
    "    path_to_ocr_output_dir = r\"{}\\{}\".format(path_to_municipality,ocr_output_files_dir_name)\n",
    "    print(\"path_to_ocr_output_dir={}\\n\".format(path_to_ocr_output_dir))\n",
    "    path_to_ocr_metadata_output_dir = r\"{}\\{}\".format(path_to_municipality,ocr_output_file_metadata_dir_name)\n",
    "    print(\"path_to_ocr_metadata_output_dir={}\\n\".format(path_to_ocr_metadata_output_dir))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    excel_output_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_name)#ocred website files\n",
    "\n",
    "    #per page and file below\n",
    "    excel_output_embedded_text_per_page_and_file_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_embedded_text_per_page_and_file_name)#original embedded text in pdfs\n",
    "    excel_output_ocr_text_per_page_and_file_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_ocr_text_per_page_and_file_name)#original embedded text in pdfs\n",
    "\n",
    "\n",
    "    #location to valid words on each pages for rotational OCR vs normal orientation OCR.\n",
    "    excel_output_valid_words_rotated_ocr_text_per_page_and_file_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_valid_words_rotated_ocr_text_per_page_and_file_name)#original embedded text in pdfs\n",
    "    excel_output_valid_words_ocr_text_per_page_and_file_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_valid_words_ocr_text_per_page_and_file_name)#original embedded text in pdfs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    excel_output_original_text_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_original_text_file_name)\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Use metadata and links as features to re-ranking/ ML model.\n",
    "    excel_metadata_output_path & excel_path_links_output_path\n",
    "    \"\"\"\n",
    "    #metadata for the pdf files\n",
    "    excel_metadata_output_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_metadata_name)\n",
    "    #Links found in PDF likely Important \n",
    "    excel_path_links_output_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_page_links)\n",
    "\n",
    "    excel_output_merged_text_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_merged_text_per_page_and_file_name)\n",
    "\n",
    "\n",
    "    excel_output_skipped_pdfs_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,skipped_pdf_file_name)\n",
    "\n",
    "    excel_output_page_orientation_stats_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_page_orientation_stats)\n",
    "    excel_output_page_orientation_stats_extra_path= r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_page_orientation_stats_extra)\n",
    "\n",
    "    #ocr results for the page being rotated or not\n",
    "    excel_output_rotated_ocr_text_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_rotated_ocr_text_name)\n",
    "    excel_output_not_rotated_ocr_text_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_file_not_rotated_ocr_text_name)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE: Fix\n",
    "    excel_output_merged_text_per_page_before_joining_to_file_path\n",
    "    Same output_excel_merged_text_final_per_page_and_file_name\n",
    "    \"\"\"\n",
    "    #results of each page of text per document prior to merging into one file per PDF\n",
    "    excel_output_merged_text_per_page_before_joining_to_file_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_merged_text_final_per_page_and_file_name)\n",
    "\n",
    "    output_excel_merged_text_final_name = \"not_used_file.xlsx\"\n",
    "    excel_output_merged_final_text_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_merged_text_final_name)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE USE FOR OCR ON PDFS \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    OrderedDict() used below to link up the page text differences from embedded and OCRED text. \n",
    "    file_original_text_dict = {}#store text embedded in the PDF file already. \n",
    "\n",
    "    file_dict = {}\n",
    "    file_metadata_dict = {}\n",
    "    \"\"\"\n",
    "\n",
    "    #OrderedDict not used or needed in current version: could be normal dict\n",
    "    file_original_text_dict = OrderedDict()#store text embedded in the PDF file already. \n",
    "    file_dict = OrderedDict()\n",
    "    file_metadata_dict = OrderedDict()\n",
    "    file_page_links_dict = defaultdict(list)#default dict with list as key. page : [links on page] \n",
    "\n",
    "    ocr_text_per_file_and_page = {}# key = file_name , value = dict(key = page number, value = text on page)\n",
    "    embedded_text_per_file_and_page = {}# key = file_name , value = dict(key = page number, value = text on page)\n",
    "\n",
    "    ocr_text_orientation_stats_per_file_and_page = {}# key = file_name , value = dict(key = page number, value = tuple( rotated words count, not rotated words count ))\n",
    "\n",
    "    #Used to save the docx and txt files into the save folder as OCRED converted PDFs\n",
    "    docx_file_dict ={}\n",
    "    txt_file_dict ={}\n",
    "\n",
    "\n",
    "    if not minimial_file_output:\n",
    "        rotated_ocr_text_per_file_and_page = {}# key = file_name , value = dict(key = page number, value = text on page)\n",
    "        not_rotated_ocr_text_per_file_and_page = {}# key = file_name , value = dict(key = page number, value = text on page)\n",
    "        rotated_ocr_text_valid_words_per_file_and_page = defaultdict(list)# key = file_name , value = dict(key = page number, value = list[valid words on page])\n",
    "        ocr_text_valid_words_per_file_and_page = defaultdict(list)# key = file_name , value = dict(key = page number, value = list[valid words on page])\n",
    "\n",
    "\n",
    "\n",
    "    skipped_pdf_dict = {}#key = file, value = page count\n",
    "    for file in os.listdir(ocr_folder_path):\n",
    "        #All the documents collected from a single webpage/municipality\n",
    "\n",
    "        print(file)\n",
    "        if \".pdf\" in file: \n",
    "            #pdf_url_path = r\"{}\\{}\".format(path_to_website_documents,file)\n",
    "            pdf_url_path = r\"{}\\{}\".format(ocr_folder_path,file)\n",
    "            #print(pdf_url_path)\n",
    "            doc = fitz.open(pdf_url_path)\n",
    "\n",
    "            if not minimial_file_output:\n",
    "                #debugging differences between word return with rotating page vs not\n",
    "                rotated_ocr_text_per_page = {}\n",
    "                not_rotated_ocr_text_per_page = {}\n",
    "                rotated_ocr_text_valid_words_per_page = defaultdict(list)# key = file_name , value = dict(key = page number, value = list[valid words on page])\n",
    "                ocr_text_valid_words_per_page = defaultdict(list)# key = file_name , value = dict(key = page number, value = list[valid words on page])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Page count check will bypass pdf if larger than ocr_page_count_limit\n",
    "            if doc.page_count > ocr_page_count_limit: \n",
    "                skipped_pdf_dict[file]=doc.page_count\n",
    "                continue\n",
    "\n",
    "\n",
    "            #Save document metadata and relate to OCR errors.\n",
    "            file_metadata_dict[file] = doc.metadata # nested dict: with  (file_name : dict(PDF arg/feature name: value))\n",
    "\n",
    "            ocr_text_per_page_dict = {}#key == page number, value = text on page\n",
    "            embedded_text_per_page_dict = {}#key == page number, value = text on page\n",
    "            ocr_text_orientation_stats_per_page_dict = {}#key == page number, value = tuple( rotated words count, not rotated words count ))\n",
    "\n",
    "\n",
    "            page_number = 1\n",
    "\n",
    "            file_text = \"\"#orced text\n",
    "            file_original_text =\"\"#embedded text\n",
    "\n",
    "\n",
    "\n",
    "            page_img_ocr_save_path = r\"{}\\{}.txt\".format(path_to_ocr_output_dir,file)\n",
    "\n",
    "            page_agg = []#each page of OCR text\n",
    "            page_agg_original_text =[]#each page of embedded text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #print(os.environ[\"TESSDATA_PREFIX\"])\n",
    "            for page in doc:#pages of pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if page_number > ocr_page_early_stop_limit:\n",
    "                    #file closed outside loop. \n",
    "                    break#break into the outter loop here thus stopping OCR on the current file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                TODO: Could include positional encoding of words with TextPage.extractDict()\n",
    "                #https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_text\n",
    "                \"\"\"\n",
    "\n",
    "                #Saving all links found on a page.\n",
    "                if len(page.get_links()) > 0:\n",
    "                    file_page_links_dict[file].append(page.get_links())\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                Check if page is not a scan(image) and instead already has text recognized in the PDF( i.e. if text is in document then don’t need to apply OCR).\n",
    "                If already text and OCR is applied then there is a chance of OCR error, especially if the text font is weird. \n",
    "                Can use this also to record OCR errors if text is in document. These errors could be made into rules to transform text if a OCRED word is incorrect.\n",
    "\n",
    "                .get_text() \n",
    "                .get_textpage()\n",
    "                \"\"\"\n",
    "\n",
    "                #\n",
    "                #original_text_page=page.get_textpage()#see if PDF is not a scan and has the text already.\n",
    "                #embedded_text=original_text_page.extractTEXT()\n",
    "                embedded_text=page.get_text()\n",
    "                page_agg_original_text.append(embedded_text)\n",
    "                embedded_text_per_page_dict[page_number] = embedded_text\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                No page rotation originally. \n",
    "                Check word counts for rotation and not rotated.\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "                text_page=page.get_textpage_ocr(flags=3, language=\"eng\", dpi=72, full=True)# full=True\n",
    "                #text_page=page.get_textpage_ocr(flags=3, language=\"eng\", dpi=72, full=False)#partial ocr, worse results can't handle color background\n",
    "                ocr_text_from_page=text_page.extractText()\n",
    "                list_of_terms=ocr_text_from_page.split()#default split is on space\n",
    "\n",
    "                #NOTE: Could use to tokenize IR inverted index\n",
    "                #replace anything not a alphabet character with nothing. Using to tokenize\n",
    "                clean_list_of_terms = [re.sub(\"[^a-zA-z]+\",\"\",term) for term in list_of_terms]#these aren't valid words.\n",
    "                clean_list_of_terms = [re.sub(\"[\\[\\]-_]+\",\"\",term) for term in clean_list_of_terms]#removing [, ], _ , -\n",
    "                clean_list_of_terms = [term.lower() for term in clean_list_of_terms]\n",
    "\n",
    "                #print(\"tokenized text:clean_list_of_terms. NOT valid words just cleaned tokens\\n\")\n",
    "                #print(set(clean_list_of_terms))\n",
    "\n",
    "\n",
    "                unique_valid_words = set()\n",
    "\n",
    "\n",
    "                valid_word_count = 0\n",
    "                #for term in list_of_terms:\n",
    "                for term in clean_list_of_terms:\n",
    "                    if term in word_set:\n",
    "                        #print(\"valid word found in not rotated page:{}\".format(term))\n",
    "                        ocr_text_valid_words_per_page[page_number].append(term)\n",
    "                        valid_word_count+= 1\n",
    "                        unique_valid_words.add(term)\n",
    "\n",
    "\n",
    "                if not minimial_file_output:\n",
    "                    not_rotated_ocr_text_per_page[page_number] = ocr_text_from_page\n",
    "\n",
    "                unique_valid_word_count = len(unique_valid_words)\n",
    "                \"\"\"\n",
    "                Doing rotation word check on each page. \n",
    "                Double compute but handles rotated PDFs\n",
    "                https://pymupdf.readthedocs.io/en/latest/page.html#Page.set_rotation\n",
    "\n",
    "                page.set_rotation(90) is inplace\n",
    "\n",
    "                rotated page below  with page.set_rotation(90)\n",
    "                \"\"\"\n",
    "                page.set_rotation(90)\n",
    "\n",
    "                #See if dpi of the file is returned from metadata or elsewhere and use to pass in here for best OCR settings.\n",
    "                #DPI not in document details.\n",
    "                #if file_metadata_dict[file][dpi] number then\n",
    "                #dpi = file_metadata_dict[file][dpi]\n",
    "                rotated_text_page= page.get_textpage_ocr(flags=3, language=\"eng\", dpi=72, full=True)\n",
    "                rotated_ocr_text_from_page=rotated_text_page.extractText()\n",
    "                list_of_rotated_terms=rotated_ocr_text_from_page.split()#default is on space\n",
    "\n",
    "                #replace anything not a alphabet character with nothing. Using to tokenize\n",
    "                clean_list_of_rotated_terms = [re.sub(\"[^a-zA-z]+\",\"\",term) for term in list_of_rotated_terms]\n",
    "                clean_list_of_rotated_terms = [re.sub(\"[\\[\\]-_]+\",\"\",term) for term in clean_list_of_rotated_terms]\n",
    "                clean_list_of_rotated_terms = [term.lower() for term in clean_list_of_rotated_terms]\n",
    "                #print(\"tokenized text:clean_list_of_rotated_terms. NOT valid words just cleaned tokens\\n\")\n",
    "                #print(set(clean_list_of_rotated_terms))\n",
    "                rotated_valid_word_count = 0\n",
    "\n",
    "\n",
    "                #Unique words could still bias the data because the OCR junk could have many short unique words.\n",
    "                unique_rotated_valid_words = set()\n",
    "                #for term in list_of_rotated_terms:\n",
    "                for term in clean_list_of_rotated_terms:\n",
    "                    if term in word_set:\n",
    "                        #print(\"valid word found in rotated page:{}\".format(term))\n",
    "                        rotated_valid_word_count+= 1\n",
    "                        rotated_ocr_text_valid_words_per_page [page_number].append(term)#valid word\n",
    "                        unique_rotated_valid_words.add(term)\n",
    "\n",
    "\n",
    "                unique_rotated_valid_word_count = len(unique_rotated_valid_words)\n",
    "\n",
    "\n",
    "                if not minimial_file_output:\n",
    "                    #Storing both rotated and not rotated OCR results per file per page as a feature\n",
    "                    rotated_ocr_text_per_page[page_number] = rotated_ocr_text_from_page\n",
    "\n",
    "\n",
    "                #print(\"rotated word count:{}\\t unrotated word count:{}\".format(rotated_valid_word_count,valid_word_count))\n",
    "\n",
    "                #Factor in unique words( set count) and word character length as weighted sum because OCR junk of 1-2 char words are overwhelming actual words when page is not rotated\n",
    "                rotated_word_percent_bias = .60\n",
    "                word_length_weight = 1#NOTE two of these variables make sure they are the same starting value or else it could bias results\n",
    "\n",
    "                #subtracted away\n",
    "                rotated_word_bias = int(rotated_valid_word_count *rotated_word_percent_bias)#to not overcount words if rotated since most documents aren't rotated. \n",
    "\n",
    "                #Weighted sum based on character count\n",
    "\n",
    "                rotated_word_char_length_dict = defaultdict(list)#key = word length, value = [words with key as word length] \n",
    "                for valid_rotated_word in rotated_ocr_text_valid_words_per_page[page_number]:\n",
    "                    key = len(valid_rotated_word)\n",
    "                    rotated_word_char_length_dict[key].append(valid_rotated_word)\n",
    "\n",
    "\n",
    "                rotated_word_char_length_weighted_value = 0\n",
    "\n",
    "                ordered_keys_rotated_word_char_length_dict = collections.OrderedDict(sorted(rotated_word_char_length_dict.items()))\n",
    "                for rotated_word_length, word_list in ordered_keys_rotated_word_char_length_dict.items():\n",
    "                #for rotated_word_length, word_list in rotated_word_char_length_dict.items():\n",
    "                    #print(\"rotated_word_char_length_dict:{}\\n\".format(rotated_word_length))\n",
    "                    word_length_weight +=.3\n",
    "                    #word_length_weight +=1 #same weight for rotated vs not, but getting too many false positives so lowering rotated weight to hopefully reduce FP\n",
    "                    rotated_word_char_length_weighted_value+= (rotated_word_length * word_length_weight)* len(word_list)\n",
    "\n",
    "\n",
    "                #not rotated words \n",
    "                word_char_length_dict = defaultdict(list)#key = word length, value = [words with key as word length] \n",
    "                for valid_word in ocr_text_valid_words_per_page[page_number]:\n",
    "                    key = len(valid_word)\n",
    "                    word_char_length_dict[key].append(valid_word)\n",
    "\n",
    "\n",
    "                word_char_length_weighted_value = 0\n",
    "                word_length_weight = 1#Needed to reset word_length_weight after rotated added to the variable above.\n",
    "\n",
    "                ordered_keys_word_char_length_dict = collections.OrderedDict(sorted(word_char_length_dict.items()))\n",
    "                for word_length, word_list in ordered_keys_word_char_length_dict.items():\n",
    "                #for word_length, word_list in word_char_length_dict.items():\n",
    "                    #print(\"word_char_length_dict:{}\\n\".format(word_length))\n",
    "\n",
    "                    #NOTE word_length_weight assumes sorted  Keys == char length,  needs to be sorted in ascending order. Otherwise higher weights could be assigned to random length words. \n",
    "                    #The longer the word the higher the weight it receives because OCR error for longer words is less likely but still possible.\n",
    "                    word_length_weight +=1#giving higher weight to longer words. Keys == char length,  needs to be sorted in ascending order\n",
    "                    word_char_length_weighted_value+= (word_length * word_length_weight)* len(word_list)\n",
    "\n",
    "\n",
    "\n",
    "                rotated_page_metric = rotated_valid_word_count - rotated_word_bias + unique_rotated_valid_word_count + rotated_word_char_length_weighted_value\n",
    "                not_rotated_page_metric = valid_word_count + unique_valid_word_count + word_char_length_weighted_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #if (rotated_valid_word_count - rotated_word_bias) > valid_word_count:#too many false positives\n",
    "                # if rotated_page_metric larger then most likely the PDF page is rotated( LHS to RHS) rather than read from top to bottom. \n",
    "\n",
    "                # if both scores are within 10 points of each other add both words for the given page.\n",
    "                degree_of_closeness = 10\n",
    "\n",
    "                #True== Use both rotated text and unrotated as the page to not lose text. False clearer winner of the correct page orientation\n",
    "                within_degree_of_closeness = abs(rotated_page_metric - not_rotated_page_metric) < degree_of_closeness\n",
    "\n",
    "\n",
    "\n",
    "                #ocr_text_orientation_stats_per_page_dict[page_number] = (rotated_valid_word_count,valid_word_count)\n",
    "                ocr_text_orientation_stats_per_page_dict[page_number] = (rotated_valid_word_count,valid_word_count,unique_rotated_valid_word_count,unique_valid_word_count,rotated_word_char_length_weighted_value,word_char_length_weighted_value,rotated_page_metric,not_rotated_page_metric,within_degree_of_closeness)\n",
    "\n",
    "\n",
    "\n",
    "                #TODO:Check if this throws off other metadata files?\n",
    "                #Add both rotated and not rotated text info for a given page as it is too close to tell the correct page orientation.\n",
    "                #if (rotated_page_metric - not_rotated_page_metric) < degree_of_closeness:\n",
    "                if within_degree_of_closeness:\n",
    "                    #print(rotated_ocr_text_from_page)\n",
    "                    page_agg.append(rotated_ocr_text_from_page)\n",
    "                    #page_agg.append(\"\\n\")#TODO: Check if only one new line needed for diff obj\n",
    "                    \"\"\"\n",
    "                    NOTE: Potential place for low quality OCR by adding both text from two orientations.\n",
    "                    \"\"\"\n",
    "                    ocr_text_per_page_dict[page_number] = rotated_ocr_text_from_page + ocr_text_from_page\n",
    "                    page_number += 1\n",
    "\n",
    "\n",
    "\n",
    "                #NOTE: Below likely includes false positives.\n",
    "                #Page is considered rotated: adding rotated text\n",
    "                elif rotated_page_metric > not_rotated_page_metric:\n",
    "                #if rotated_page_metric > not_rotated_page_metric:\n",
    "                    #print(rotated_ocr_text_from_page)\n",
    "                    page_agg.append(rotated_ocr_text_from_page)\n",
    "                    #page_agg.append(\"\\n\")#TODO: Check if only one new line needed for diff obj\n",
    "                    ocr_text_per_page_dict[page_number] = rotated_ocr_text_from_page\n",
    "                    page_number += 1\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    #more words not rotated according to orientation metric so use the unrotated PDF OCRED page text \n",
    "                    #print(ocr_text_from_page)\n",
    "                    page_agg.append(ocr_text_from_page)\n",
    "                    ocr_text_per_page_dict[page_number] = ocr_text_from_page\n",
    "                    #page_agg.append(\"\\n\")\n",
    "                    page_number += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #Document/PDF file done, join text and save into dict\n",
    "\n",
    "            \"\"\"\n",
    "            Document/PDF file done, join text and save into dict\n",
    "            Note: file_text, file_dict not needed anymore.  \n",
    "            Using embedded_text_per_file_and_page & ocr_text_orientation_stats_per_file_and_page \n",
    "            for more complex joins later on in program.\n",
    "            \"\"\"\n",
    "            #file_text=file_text.replace(u\"\\ufffd\", \"*\")#replace \n",
    "            file_text = \" \".join(page_agg)#joining strings from each page all at once, at the end.\n",
    "            file_dict[file] = file_text\n",
    "\n",
    "            #Save each page so joins are page based: this way can support PDF with scan pages & embedded text pages\n",
    "            ocr_text_per_file_and_page[file]= ocr_text_per_page_dict\n",
    "\n",
    "            embedded_text_per_file_and_page[file] = embedded_text_per_page_dict\n",
    "\n",
    "\n",
    "            ocr_text_orientation_stats_per_file_and_page[file]= ocr_text_orientation_stats_per_page_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if not minimial_file_output:\n",
    "                rotated_ocr_text_per_file_and_page[file] = rotated_ocr_text_per_page# key = file_name , value = dict(key = page number, value = text on page)\n",
    "                not_rotated_ocr_text_per_file_and_page[file] = not_rotated_ocr_text_per_page# key = file_name , value = dict(key = page number, value = text on page)\n",
    "                rotated_ocr_text_valid_words_per_file_and_page[file] = rotated_ocr_text_valid_words_per_page\n",
    "                ocr_text_valid_words_per_file_and_page[file] = ocr_text_valid_words_per_page\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            file_original_text = \" \".join(page_agg_original_text)\n",
    "            file_original_text_dict[file] = file_original_text\n",
    "            doc.close()\n",
    "\n",
    "        elif \".docx\" in file:\n",
    "            \"\"\"\n",
    "            TODO: Verify that metadata files and other files are being applied to .docx & .txt as well--where relevant.\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "            Not a scan, text already in format computer understands: i.e. OCR not needed. \n",
    "            Read in text and output to file.\n",
    "            \"\"\"\n",
    "            #word_url_path = r\"{}\\{}\".format(path_to_website_documents,file)\n",
    "            word_url_path = r\"{}\\{}\".format(ocr_folder_path,file)\n",
    "\n",
    "            file_text=docx2txt.process(word_url_path)\n",
    "            file_dict[file] = file_text#old use docx_file_dict[file]\n",
    "\n",
    "            docx_file_dict[file] = file_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif \".txt\" in file:\n",
    "\n",
    "            \"\"\"\n",
    "            Not a scan, text already in format computer understands: i.e. OCR not needed. \n",
    "            Read in text and output to file.\n",
    "            \"\"\"\n",
    "            #path_to_doc = r\"{}\\{}\".format(path_to_website_documents,file)\n",
    "            path_to_doc = r\"{}\\{}\".format(ocr_folder_path,file)\n",
    "            \n",
    "            #txt_file=open(path_to_doc,\"r+\")\n",
    "            #txt_file=open(path_to_doc,\"r+\",encoding='utf-8', errors='ignore')\n",
    "            txt_file=open(path_to_doc,\"r+\",encoding='utf-8', errors='surrogateescape')\n",
    "            \n",
    "\n",
    "            all_lines_of_txt_file=txt_file.read()#string for each file.\n",
    "            file_dict[file] = all_lines_of_txt_file#old use txt_file_dict[file]\n",
    "            txt_file.close()\n",
    "\n",
    "            txt_file_dict[file] = all_lines_of_txt_file\n",
    "\n",
    "        else:\n",
    "            print(\"Not supported file type:{}\".format(file))\n",
    "\n",
    "\n",
    "    #printing OCR warning. \n",
    "    print(fitz.TOOLS.mupdf_warnings())\n",
    "    #End of apply OCR to a single municipality\n",
    "\n",
    "    \"\"\"\n",
    "    Saving results of applying OCR to folder of files.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Saving metadata from PDF file.\n",
    "    pdf_field == Title useful e.g. \"WORKERS’ COMPENSATION REQUIREMENTS UNDER WCL SECTION  57\"\n",
    "    ('Excavation Application (1).pdf', 'title')\tSTREET OPENING PERMIT APPLICATION\n",
    "\n",
    "     'subject' might be useful: doesn't seem to be filled often.\n",
    "\n",
    "\n",
    "    ('clergy_exemption_form.pdf', 'subject')\tApplication for partial tax exemption for real property of members of the clergy\n",
    "    ('clergy_exemption_form.pdf', 'keywords')\tApplication for partial tax exemption for real property of members of the clergy\n",
    "\n",
    "    Could relate 'creator' & 'producer' to OCR results. \n",
    "    ('Employment Application - Lake Isle2020.pdf', 'creator')\tMicrosoft® Word 2013\n",
    "    ('Employment Application - Lake Isle2020.pdf', 'producer')\tMicrosoft® Word 2013\n",
    "\n",
    "    'keywords' useful when provided\n",
    "    ('GrievanceFormRP524.pdf', 'keywords')\t\"Complaint, Real Property Assessment, Board of Assessment Review\"\n",
    "\n",
    "    ('mv6641 (1).pdf', 'title')\tHow To Apply For A Parking Permit Or License Plates For Persons With Severe Disabilities \n",
    "    ('mv6641 (1).pdf', 'author')\tNew York State Department of Motor Vehicles\n",
    "    ('mv6641 (1).pdf', 'subject')\tApplication For A Parking Permit Or License Plates, For Persons With Severe Disabilities\n",
    "    ('mv6641 (1).pdf', 'keywords')\tHow, To, Apply, For, A, Parking, Permit, Or, License, Plates, Persons, With, Severe, Disabilities, Application, New, York, State, Department, of, Motor, Vehicles\n",
    "\n",
    "    ('rp425b_fill_in 2018.pdf', 'title')\tForm RP-425-B:6/18:Application for Basic STAR Exemption for the 2019-2020 School Year:rp425b\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    metadata_df=pd.DataFrame.from_dict({(i,j): file_metadata_dict[i][j] \n",
    "                               for i in file_metadata_dict.keys() \n",
    "                               for j in file_metadata_dict[i].keys()},\n",
    "                           orient='index', columns=['pdf_field_value'])\n",
    "\n",
    "    #Don't need to save below: redundant info. Unpack tuple in index into columns\n",
    "    #metadata_df['file_name'], metadata_df['pdf_field'] = zip(*metadata_df.index)\n",
    "\n",
    "    metadata_df.index.name = \"file name and PDF field\"\n",
    "\n",
    "\n",
    "    metadata_df.to_excel(excel_metadata_output_path)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Saving PDF page links. \n",
    "\n",
    "    converting to df then exporting\n",
    "\n",
    "    https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_text\n",
    "\n",
    "    kind values below\n",
    "    https://pymupdf.readthedocs.io/en/latest/vars.html#linkdest-kinds\n",
    "\n",
    "    Saving page links found in the PDF. Check these links/use as a feature/context to the model.\n",
    "\n",
    "    'uri': 'mailto:PDtraffic@eastchester.org' might be needed reference to contact?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #file_page_links_dict\n",
    "\n",
    "\n",
    "    #format\n",
    "    #page_links=pd.Series(file_page_links_dict).rename_axis('key_column').reset_index(name='value_column')\n",
    "    page_links=pd.Series(file_page_links_dict).rename_axis('file_name').reset_index(name='link_info')\n",
    "\n",
    "    #print(page_links)\n",
    "    page_links_df=pd.DataFrame(page_links)\n",
    "\n",
    "    page_links_df.to_excel(excel_path_links_output_path)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    NOT needed more complex joins used in merged_text_final.\n",
    "    Nevermind need to use file_dict because the other word and .txt files use this\n",
    "    \"\"\"\n",
    "\n",
    "    #converting ocred file text into needed format: to pandas df then writing to excel file.\n",
    "    #file_dict[file]\n",
    "    #Conversion of the whole file and join rather than the individual  pages actually used.\n",
    "\n",
    "    df = pd.DataFrame.from_dict(file_dict,orient=\"index\",columns=[\"text\"])\n",
    "    df.index.name = \"file_name\"\n",
    "\n",
    "    #Removing any formulas( cell starting with =) so excel file doesn't error.\n",
    "    df['text'].replace(to_replace='^=', value=' =',regex=True, inplace=True)\n",
    "    #if not old_output:\n",
    "\n",
    "    if not old_output:\n",
    "    #if not minimial_file_output:\n",
    "        df.to_excel(excel_output_path)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Saving embedded text from the PDF files. \n",
    "    Use this text over the OCRED text. Can check if the text feature is Nan, if so then use OCR field.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #original text from file into needed format: to pandas df then writing to excel file.\n",
    "    #file_dict[file]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(file_original_text_dict,orient=\"index\",columns=[\"text\"])\n",
    "    df.index.name = \"file_name\"\n",
    "\n",
    "    #Removing any formulas( cell starting with =) so excel file doesn't error.\n",
    "    df['text'].replace(to_replace='^=', value=' =',regex=True, inplace=True)\n",
    "\n",
    "    if not old_output:\n",
    "    #if not minimial_file_output:\n",
    "        df.to_excel(excel_output_original_text_path)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Checking results from OCR rotated text vs not. These are different from 'orientation_metric_ocred_page_text'. \n",
    "\n",
    "    'orientation_metric_ocred_page_text' determines which OCR results( rotated, not, or both because too close to tell) are to be used. \n",
    "\n",
    "    rotated_ocr_text_per_file_and_page[file] = rotated_ocr_text_per_page# key = file_name , value = dict(key = page number, value = text on page)\n",
    "    not_rotated_ocr_text_per_file_and_page[file] = not_rotated_ocr_text_per_page# key = file_name , value = dict(key = page number, value = text on page)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    see_not_joined_files = False\n",
    "\n",
    "    if not minimial_file_output:\n",
    "        rotated_ocr_text_per_file_and_page_df=pd.DataFrame.from_dict({(i,j): rotated_ocr_text_per_file_and_page[i][j] \n",
    "                                   for i in rotated_ocr_text_per_file_and_page.keys() \n",
    "                                   for j in rotated_ocr_text_per_file_and_page[i].keys()},\n",
    "                               orient='index', columns=['rotated_ocr_page_text'])#rotated_ocr_text 'page_text'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        not_rotated_ocr_text_per_file_and_page_df=pd.DataFrame.from_dict({(i,j): not_rotated_ocr_text_per_file_and_page[i][j] \n",
    "                                   for i in not_rotated_ocr_text_per_file_and_page.keys() \n",
    "                                   for j in not_rotated_ocr_text_per_file_and_page[i].keys()},\n",
    "                               orient='index', columns=['not_rotated_ocred_page_text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        rotated_ocr_text_per_file_and_page_df['rotated_ocr_page_text'].replace(to_replace='^=', value=' =',regex=True, inplace=True)\n",
    "\n",
    "        rotated_ocr_text_per_file_and_page_df['file_name'],rotated_ocr_text_per_file_and_page_df['page_number'] = zip(*rotated_ocr_text_per_file_and_page_df.index)\n",
    "        #Reset index to get a random index and then be able to do the merging.\n",
    "        rotated_ocr_text_per_file_and_page_df.reset_index( drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        if see_not_joined_files:\n",
    "            #NOTE: Could remove individual write out as data in page_orientation_stats_extra.xlsx\n",
    "            rotated_ocr_text_per_file_and_page_df.to_excel(excel_output_rotated_ocr_text_path)\n",
    "\n",
    "\n",
    "        not_rotated_ocr_text_per_file_and_page_df['not_rotated_ocred_page_text'].replace(to_replace='^=', value=' =',regex=True, inplace=True)\n",
    "        not_rotated_ocr_text_per_file_and_page_df['file_name'],not_rotated_ocr_text_per_file_and_page_df['page_number'] = zip(*not_rotated_ocr_text_per_file_and_page_df.index)\n",
    "\n",
    "        not_rotated_ocr_text_per_file_and_page_df.reset_index( drop=True, inplace=True)\n",
    "        if see_not_joined_files:\n",
    "            #NOTE: Could remove individual write out as data in page_orientation_stats_extra.xlsx\n",
    "            not_rotated_ocr_text_per_file_and_page_df.to_excel(excel_output_not_rotated_ocr_text_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    NOTE: Wouldn't have to store these two dfs alone, could always store the joined result.\n",
    "    Save below or just use to join ocr text per page\n",
    "\n",
    "    page_text == embedded_text already stored in PDF file\n",
    "\n",
    "    'merged_text' == column used to blend the PDFs pages together into one file as .txt file.\n",
    "\n",
    "    1: Check for Nan values in embedded_text_per_file_and_page_df. If Nan then merge with whatever is in ocr_text_per_file_and_page\n",
    "\n",
    "    2: Check for set differences in embedded_text to see if there is a part of a page which is a scan/image which the OCR caught\n",
    "    If OCR caught then add that to the page.\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embedded_text_per_file_and_page_df=pd.DataFrame.from_dict({(i,j): embedded_text_per_file_and_page[i][j] \n",
    "                               for i in embedded_text_per_file_and_page.keys() \n",
    "                               for j in embedded_text_per_file_and_page[i].keys()},\n",
    "                           orient='index', columns=['page_text'])\n",
    "\n",
    "\n",
    "\n",
    "    #result of applying the page orientation metric in deciding if the page is rotated or not.\n",
    "    #This determines which OCR results( rotated, not, or both because too close to tell) are to be used. \n",
    "    ocr_text_per_file_and_page_df=pd.DataFrame.from_dict({(i,j): ocr_text_per_file_and_page[i][j] \n",
    "                               for i in ocr_text_per_file_and_page.keys() \n",
    "                               for j in ocr_text_per_file_and_page[i].keys()},\n",
    "                           orient='index', columns=['orientation_metric_ocred_page_text'])\n",
    "    #\n",
    "\n",
    "\n",
    "    #errors with excel it thinks there is a formula \n",
    "    embedded_text_per_file_and_page_df['page_text'].replace(to_replace='^=', value=' =',regex=True, inplace=True)\n",
    "\n",
    "    embedded_text_per_file_and_page_df['file_name'],embedded_text_per_file_and_page_df['page_number'] = zip(*embedded_text_per_file_and_page_df.index)\n",
    "    #Reset index to get a random index and then be able to do the merging.\n",
    "    embedded_text_per_file_and_page_df.reset_index( drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #Verify this should handle Plumbing-Permit-1.pdf page 2 and use its OCR. It does.\n",
    "    embedded_text_per_file_and_page_df.loc[:,'use_ocr']= embedded_text_per_file_and_page_df.apply(use_ocr_text, axis=1)\n",
    "\n",
    "    #if not minimial_file_output:\n",
    "    if see_not_joined_files:\n",
    "        embedded_text_per_file_and_page_df.to_excel(excel_output_embedded_text_per_page_and_file_path)\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE: orientation_metric_ocred_page_text is only provided for use_ocr_x == True due to join. \n",
    "    \"\"\"\n",
    "    ocr_text_per_file_and_page_df['orientation_metric_ocred_page_text'].replace(to_replace='^=', value=' =',regex=True, inplace=True)\n",
    "    ocr_text_per_file_and_page_df['file_name'],ocr_text_per_file_and_page_df['page_number'] = zip(*ocr_text_per_file_and_page_df.index)\n",
    "\n",
    "    ocr_text_per_file_and_page_df.reset_index( drop=True, inplace=True)\n",
    "\n",
    "    #if not minimial_file_output:\n",
    "    if see_not_joined_files:\n",
    "        ocr_text_per_file_and_page_df.to_excel(excel_output_ocr_text_per_page_and_file_path)\n",
    "\n",
    "\n",
    "    # Case 1: Check for Nan values in embedded_text_per_file_and_page_df. If Nan then merge with whatever is in ocr_text_per_file_and_page\n",
    "    #join OCR page text on embedded_text_empty_pages\n",
    "    embedded_text_empty_pages=embedded_text_per_file_and_page_df.loc[embedded_text_per_file_and_page_df['use_ocr'], ]\n",
    "\n",
    "    #Note page_text blank on this due to setup and could be removed as column.\n",
    "    # Good test example is '2017-Policy-and-Application-Town-Co-Sponsorship-Events-3.pdf'\n",
    "    merged_ocr_text=pd.merge(embedded_text_empty_pages.copy(),ocr_text_per_file_and_page_df, how='left', on=['page_number','file_name'])\n",
    "\n",
    "    #Removing suffix by not including in merge\n",
    "    #merged_ocr_text=pd.merge(embedded_text_empty_pages[,['page_number','file_name']].copy(),ocr_text_per_file_and_page_df, how='left', on=['page_number','file_name'])\n",
    "\n",
    "\n",
    "\n",
    "    #Empty embedded text pages will use whatever the OCR captured.\n",
    "    merged_ocr_text['merged_text'] = merged_ocr_text['orientation_metric_ocred_page_text']\n",
    "    #print(\"merged_ocr_text columns:{}\".format(merged_ocr_text.columns))\n",
    "\n",
    "    if not old_output:\n",
    "    #if not minimial_file_output:\n",
    "        merged_ocr_text.to_excel(excel_output_merged_text_path)\n",
    "\n",
    "\n",
    "\n",
    "    merged_text_final=pd.merge(embedded_text_per_file_and_page_df, merged_ocr_text,how='left', on=['file_name','page_number'])\n",
    "\n",
    "\n",
    "    #print(merged_text_final.columns)\n",
    "    #NOTE: _x suffix just denotes leftmost df columns which are common to the other df used in join. _y is rightmost df columns \n",
    "    #NOTE: Program currently only supports using embedded text xor OCR text for a single page--can't use both or combine the results of both with below.\n",
    "    # if 'use_ocr_x' == False then use the embedded text for that row as merged_text column\n",
    "    #merged_text_final.loc[merged_text_final['use_ocr_x'] == False,'merged_text'] = merged_text_final.loc[merged_text_final['use_ocr_x'] == False,'page_text_x']#not joining\n",
    "    merged_text_final.loc[merged_text_final['use_ocr_x'] == False,'merged_text'] = merged_text_final['page_text_x']\n",
    "    #merged_text_final.loc[merged_text_final['merged_text'].isnull(),'merged_text'] = merged_text_final.loc[merged_text_final['page_text_x'].notnull(),'page_text']\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    32,767 characters\n",
    "    Microsoft Excel has a character limit of 32,767 characters in each cell.\n",
    "\n",
    "    The excel file wasn't showing all of the text in a given cell but if one double clicks on the cell then more shows up. \n",
    "    TODO: Load in and try to print and see if all the text prints or if excel limit comes into play.\n",
    "    Could always just save the files as plain text and load in when needed if excel loses part of the string due to char limit \n",
    "    \"\"\"\n",
    "\n",
    "    #show all text for a given document in the 'all_pages_merged' column\n",
    "    #merged_text_final['all_pages_merged'] = merged_text_final[['file_name','merged_text']].groupby('file_name')['merged_text'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "    #debugging\n",
    "    if not old_output:\n",
    "    #if not minimial_file_output:\n",
    "        merged_text_final.to_excel(excel_output_merged_text_per_page_before_joining_to_file_path)\n",
    "\n",
    "\n",
    "    #merge all pages into one df. Need 'ocred_page_text' + 'page_text' \n",
    "    #merged_text_final.groupby(\"file_name\")['text'].apply(list)\n",
    "\n",
    "    #Keeping as a feature \n",
    "    #merged_text_final.groupby(\"file_name\", as_index = False).agg({'merged_text': list})\n",
    "    #merged_text_final.groupby(\"file_name\", as_index = True).agg({'merged_text': list})\n",
    "\n",
    "\n",
    "    #Here the ' ' would be the char joining pages of the PDF\n",
    "    #Apply below to make sure pages are coming out in correct order\n",
    "    merged_text_final.sort_values(by = [\"file_name\",\"page_number\"], inplace=True)\n",
    "    document_pages_merged=merged_text_final.groupby(\"file_name\")['merged_text'].apply(' '.join).reset_index()\n",
    "    #document_pages_merged=merged_text_final.groupby(\"file_name\")['merged_text'].apply(' '.join).reset_index().copy()\n",
    "    #print(merged_text_final.groupby(\"file_name\")['merged_text'].apply(' '.join))\n",
    "    #merged_text_final.groupby(\"file_name\")['merged_text'].apply(' '.join).reset_index().to_excel(excel_output_merged_final_text_path)#only joins first two pages. \n",
    "    #document_pages_merged=merged_text_final.groupby(\"file_name\")['merged_text'].apply(' '.join).reset_index()\n",
    "\n",
    "    #\n",
    "    see_truncation = False\n",
    "    if see_truncation:\n",
    "        document_pages_merged.to_excel(excel_output_merged_final_text_path)#DON'T DO THIS truncates past char limit\n",
    "    #merged_text_final.to_excel(excel_output_merged_final_text_path)\n",
    "    #TODO: Case 2 check if anything is in the OCR page which isn't in the embedded text page. Might not need this case. Haven't seen an example of this with embedded text and then a scan/image of text on the same page. Have seen a logo though as a scan and text. \n",
    "    \"\"\"\n",
    "    for key in document_pages_merged:\n",
    "        print(\"Group:{}\\tValues:{}\".format(key,document_pages_merged.get_group(key)))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Write each row of document_pages_merged( a PDF file from website) to a file to avoid the char limit of excel.\n",
    "\n",
    "    This plain text files will be the input for the tokenizer and model.Need to put them into df. \n",
    "\n",
    "    .pdf.txt is file name change unless file_name_wo_extension is used.\n",
    "\n",
    "    Plain text and word documents should also be written to this folder as plain text.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, row in document_pages_merged.iterrows():\n",
    "        file_merged_text = '{}'.format(row.merged_text)\n",
    "        file_name_wo_extension=row.file_name.replace(\".pdf\",\"\")\n",
    "\n",
    "        #print(\"Row columns:{}\".format(row.columns))\n",
    "        #print(row)\n",
    "        #NOTE: downstream in program will attempt to match on file name notice the .txt is added to the end of the .pdf\n",
    "        #Could remove .pdf then match on file name without extension\n",
    "        #file_name_wo_extension=row.file_name.replace(\".pdf\",\"\")\n",
    "        #with open(r'{}\\{}.txt'.format(path_to_ocr_output_dir, file_name_wo_extension), 'w',errors='surrogateescape') as text_file:\n",
    "\n",
    "        #TODO: Check if unknown chars are in the individual output files \n",
    "        #Note  errors='ignore' writes out but then keeps the char in the file. Attempting other args for errors\n",
    "        #with open(r'{}\\{}.txt'.format(path_to_ocr_output_dir, row.file_name), 'w',errors='replace') as text_file:\n",
    "        with open(r'{}\\{}.txt'.format(path_to_ocr_output_dir, file_name_wo_extension), 'w',errors='backslashreplace') as text_file:\n",
    "\n",
    "            text_file.write(file_merged_text)\n",
    "            text_file.close()\n",
    "\n",
    "    #Writing out docx files to the same folder as the converted PDFs to text & .txt files\n",
    "    for file_name, text in docx_file_dict.items():\n",
    "        file_name_wo_extension=file_name.replace(\".docx\",\"\")\n",
    "        #with open(r'{}\\{}.txt'.format(path_to_ocr_output_dir, file_name), 'w',errors='replace') as text_file:\n",
    "        with open(r'{}\\{}.txt'.format(path_to_ocr_output_dir, file_name_wo_extension), 'w',errors='backslashreplace') as text_file:\n",
    "\n",
    "            text_file.write(text)\n",
    "            text_file.close()\n",
    "\n",
    "\n",
    "    #Writing out txt files to the same folder as the converted PDFs to text & docx files\n",
    "    for file_name, text in txt_file_dict.items():\n",
    "\n",
    "        with open(r'{}\\{}.txt'.format(path_to_ocr_output_dir, file_name), 'w',errors='backslashreplace') as text_file:\n",
    "\n",
    "            text_file.write(text)\n",
    "            text_file.close()\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    File denoting skipped pdfs due to page length limit\n",
    "    \"\"\"\n",
    "    skipped_pdfs = pd.DataFrame.from_dict(skipped_pdf_dict,orient=\"index\",columns=[\"page_count\"])\n",
    "    skipped_pdfs.index.name = \"file_name\"\n",
    "    skipped_pdfs.to_excel(excel_output_skipped_pdfs_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Stats on which text was used for a given page: rotated page or not, and word counts for those pages.\n",
    "\n",
    "    ocr_text_orientation_stats_per_file_and_page[file]\n",
    "\n",
    "    ocr_text_orientation_stats_per_page_dict[page_number] = (rotated_valid_word_count,valid_word_count)\n",
    "\n",
    "\n",
    "                ocr_text_orientation_stats_per_page_dict[page_number] = (rotated_valid_word_count,valid_word_count,unique_rotated_valid_word_count,unique_valid_word_count,rotated_word_char_length_weighted_value,word_char_length_weighted_value,rotated_page_metric,not_rotated_page_metric,within_degree_of_closeness)\n",
    "\n",
    "    valid_word_count == not rotated valid word count\n",
    "    rotated_valid_word_count == rotated page and applied ocr to it, and counted valid words from brown nlp dataset\n",
    "    \"\"\"\n",
    "    #With 'within_degree_of_closeness'\n",
    "    ocr_text_columns = ['rotated_valid_word_count','valid_word_count','unique_rotated_valid_word_count','unique_valid_word_count','rotated_word_char_length_weighted_value','word_char_length_weighted_value','rotated_page_metric','not_rotated_page_metric','within_degree_of_closeness']\n",
    "\n",
    "    #Without 'within_degree_of_closeness' incase joining rotated and original is a bad idea. \n",
    "    #ocr_text_columns = ['rotated_valid_word_count','valid_word_count','unique_rotated_valid_word_count','unique_valid_word_count','rotated_word_char_length_weighted_value','word_char_length_weighted_value','rotated_page_metric','not_rotated_page_metric','within_degree_of_closeness']\n",
    "\n",
    "\n",
    "    text_orientation_stats=pd.DataFrame.from_dict({(i,j): ocr_text_orientation_stats_per_file_and_page[i][j] \n",
    "                               for i in ocr_text_orientation_stats_per_file_and_page.keys() \n",
    "                               for j in ocr_text_orientation_stats_per_file_and_page[i].keys()},\n",
    "                           orient='index', columns=ocr_text_columns)\n",
    "\n",
    "\n",
    "    #text_orientation_stats['used_rotated_page'] = text_orientation_stats['rotated_page_metric'] > text_orientation_stats['not_rotated_page_metric']\n",
    "\n",
    "    #text_orientation_stats.to_excel(excel_output_page_orientation_stats_path)\n",
    "\n",
    "    #TODO: Join the used_ocr on this to see if there was not embedded text on the page and if the system had to depend on the rotation being correct.\n",
    "    #In embedded_text_per_page but will need to make the features file_name & page_number into tuple and join on index of this.\n",
    "\n",
    "    text_orientation_stats['file_name'],text_orientation_stats['page_number'] = zip(*text_orientation_stats.index)\n",
    "\n",
    "\n",
    "\n",
    "    #Shows which pages got used for OCR debugging\n",
    "    text_orientation_stats_used_pages=pd.merge(merged_text_final, text_orientation_stats,how='left', on=['file_name','page_number'])\n",
    "    #OCR is needed and rotated page has a higher metric.\n",
    "    text_orientation_stats_used_pages.loc[text_orientation_stats_used_pages['use_ocr_x']== True,'used_rotated_page_ocr'] = text_orientation_stats_used_pages['rotated_page_metric'] > text_orientation_stats_used_pages['not_rotated_page_metric']\n",
    "\n",
    "\n",
    "    #text_orientation_stats_used_pages.loc[text_orientation_stats_used_pages['page_text_x'].isnull() and text_orientation_stats_used_pages['ocred_page_text'].isnull(),'no_text_ocr_or_embedded'] = True\n",
    "    #Seeing if both columns are Nan and if they are then set to True. Will also check if the columns are the same in general, but the code shouldn't allow this to be the case beyond the Nan Example.\n",
    "    #NOTE better way to do below.\n",
    "    text_orientation_stats_used_pages['no_text_ocr_or_embedded'] = text_orientation_stats_used_pages.apply(lambda row: True if str(row['page_text_x']) == str(row['orientation_metric_ocred_page_text']) else False, axis=1)\n",
    "\n",
    "    if not old_output:\n",
    "        text_orientation_stats_used_pages.to_excel(excel_output_page_orientation_stats_path)\n",
    "\n",
    "    #merging page OCR text rotated and not rotated into this df.  All page stats in one place.\n",
    "    #merging OCR rotated and not rotated into page stats\n",
    "    text_orientation_stats_used_pages_merged_condensed=pd.merge(text_orientation_stats_used_pages,not_rotated_ocr_text_per_file_and_page_df ,how='left', on=['file_name','page_number'])\n",
    "    text_orientation_stats_used_pages_merged_wo_embedded=pd.merge(text_orientation_stats_used_pages_merged_condensed,rotated_ocr_text_per_file_and_page_df ,how='left', on=['file_name','page_number'])\n",
    "    text_orientation_stats_used_pages_merged_wo_embedded.to_excel(excel_output_page_orientation_stats_extra_path)\n",
    "\n",
    "\n",
    "    #merging embedded text into page stats. Don't need because original leftmost df has embedded text in it\n",
    "    #CHECK do I not need embedded because in the original df in column page_text_x?\n",
    "    #text_orientation_stats_used_pages_merged_full=pd.merge(text_orientation_stats_used_pages_merged_wo_embedded, embedded_text_per_file_and_page_df,how='left', on=['file_name','page_number'])\n",
    "    #Should be all the per page details for all files. Text limit shouldn't matter because a page is unlikely to go over the 35k char limit per excel cell.\n",
    "    #text_orientation_stats_used_pages_merged_full.to_excel(excel_output_page_orientation_stats_extra_path)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Saving out valid words( in Brown nlp corpus/ nltk) per file per page.\n",
    "\n",
    "    Debug structure for valid words to ensure OCR page orientation is operating correctly.\n",
    "\n",
    "    rotated_ocr_text_valid_words_per_file_and_page \n",
    "    ocr_text_valid_words_per_file_and_page \n",
    "\n",
    "    excel_output_valid_words_rotated_ocr_text_per_page_and_file_path\n",
    "    excel_output_valid_words_ocr_text_per_page_and_file_path = r\"{}\\{}\".format(path_to_ocr_metadata_output_dir,output_excel_valid_words_ocr_text_per_page_and_file_name)#original embedded text in pdfs\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #print(rotated_ocr_text_valid_words_per_file_and_page)\n",
    "\n",
    "\n",
    "    if not minimial_file_output:\n",
    "        valid_words_rotated_ocr_text=pd.DataFrame.from_dict({(i,j): rotated_ocr_text_valid_words_per_file_and_page[i][j] \n",
    "                                   for i in rotated_ocr_text_valid_words_per_file_and_page.keys() \n",
    "                                   for j in rotated_ocr_text_valid_words_per_file_and_page[i].keys()},\n",
    "                               orient='index')\n",
    "\n",
    "        valid_words_rotated_ocr_text.index.name = \"file name and page number\"\n",
    "\n",
    "        valid_words_rotated_ocr_text.to_excel(excel_output_valid_words_rotated_ocr_text_per_page_and_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        valid_words_ocr_text=pd.DataFrame.from_dict({(i,j): ocr_text_valid_words_per_file_and_page[i][j] \n",
    "                                   for i in ocr_text_valid_words_per_file_and_page.keys() \n",
    "                                   for j in ocr_text_valid_words_per_file_and_page[i].keys()},\n",
    "                               orient='index')\n",
    "        valid_words_ocr_text.index.name = \"file name and page number\"\n",
    "\n",
    "\n",
    "        valid_words_ocr_text.to_excel(excel_output_valid_words_ocr_text_per_page_and_file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c7369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR conversion done in: 222.8482 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"OCR conversion done in: {end_time - start_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03eeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655e5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e114b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
